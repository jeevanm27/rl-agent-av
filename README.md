# ğŸš— SafeLane: Adaptive Safety-Driven RL Agent for Lane Changing

![SafeLane Demo](output.mp4)

> **Authors**: Jeevan M, M. Sridevi, Darshan R  
> ğŸ“„ **Paper**: *Adaptive Safety-Driven Deep Q-Network for Autonomous Lane Changing in Highway Environments*  
> ğŸ§ª Simulation: `highway-env` | ğŸ§  Model: Deep Q-Network (DQN) with Adaptive Rewards

---

## ğŸ“Œ Abstract

SafeLane is a Deep Reinforcement Learning-based autonomous lane-changing agent designed to balance **efficiency** and **safety** in highway scenarios. Unlike standard DQNs that use fixed reward functions, SafeLane employs an **adaptive reward shaping mechanism** that penalizes unsafe behaviors like tailgating and abrupt lane switches, while encouraging smooth, context-aware decisions.

âœ… Built using [`highway-env`](https://github.com/eleurent/highway-env)  
âœ… Implements custom DQN with safety-aware rewards  
âœ… Trained and evaluated over 5000 episodes in dynamic traffic conditions  
âœ… Final agent shows reduced collisions and better generalization  
âœ… Output demo video included (`output.mp4`)  
âœ… We can continue training from the saved model and buffer by extending the number of episodes

---

## ğŸ“ Project Structure

.
â”œâ”€â”€ final_model.py               # Training logic and DQN model
â”œâ”€â”€ final_simulation.py          # Simulation script for inference
â”œâ”€â”€ output.mp4                   # Sample simulation output
â”œâ”€â”€ research paper.docx          # Research paper with methods and results
â”œâ”€â”€ models/                      # Saved model and training data
â”‚   â”œâ”€â”€ q_network.weights.h5
â”‚   â”œâ”€â”€ target_network.weights.h5
â”‚   â”œâ”€â”€ replay_buffer.pkl
â”‚   â””â”€â”€ training_metadata.json
â””â”€â”€ README.md                    # This file

ğŸ§  Highlights
Adaptive Reward Function:

r_t = R_progress + R_safety + R_comfort

Dynamic feedback based on:

Nearby vehicle proximity

Lane-change smoothness

Rule violations and density

Deep Q-Network Setup:

Multi-Layer Perceptron (MLP)

Double DQN to reduce overestimation

Prioritized Experience Replay

Epsilon-Greedy exploration with decay

Soft target updates for training stability

Environment:

highway-v0 from highway-env

Discrete action space: ["stay", "left", "right"]

Randomized traffic conditions with realistic driving dynamics

ğŸš€ Getting Started

ğŸ”§ Installation

git clone https://github.com/jeevanm27/rl-agent-av.git
cd rl-agent-av

python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

pip install tensorflow>=2.13.0 numpy>=1.24.0 gymnasium>=0.29.0 highway-env>=1.8.0

ğŸ Training (from scratch or resume)

python final_model.py

âœ… To resume training, ensure the following files exist in the models/ directory:

q_network.weights.h5

target_network.weights.h5

replay_buffer.pkl

training_metadata.json

ğŸ® Run Simulation (evaluate agent)
python final_simulation.py

ğŸ“Š Training Results
Episode Range	Mean Reward
1000	54
2000	108
3000	126
4000	144
5000	150

ğŸ§ª Ablation Study Summary
Component Removed	Observed Effect
Adaptive Reward	Increased collisions, unstable policy
Prioritized Replay	Slower convergence, less stability
Multi-objective Reward	Over-focus on speed/lane, less safe

ğŸ“ Research Paper
ğŸ“„ research paper.docx â€” contains methodology, literature review, equations, training graphs, and references.

ğŸ’¾ Checkpoints
ğŸ“‚ models/ directory contains all necessary files to resume training:

q_network.weights.h5 â€” main Q-network

target_network.weights.h5 â€” target network

replay_buffer.pkl â€” experience replay data

training_metadata.json â€” training progress, rewards


ğŸ§  Citation
If you use this project in your research, please cite:
@article{jeevan2025safelane,
  title={Adaptive Safety-Driven Deep Q-Network for Autonomous Lane Changing in Highway Environments},
  author={Jeevan M and M. Sridevi and Darshan R},
  journal={Under Review},
  year={2025}
}
ğŸ“¬ Contact
ğŸ“§ Email: jeevan231227@gmail.com
ğŸ”— GitHub: @jeevanm27
